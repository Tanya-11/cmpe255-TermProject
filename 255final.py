# -*- coding: utf-8 -*-
"""255Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qn1mkTOfP_tqkvfO4Iz44K_cUmmVZ8Nw
"""

# Commented out IPython magic to ensure Python compatibility.
# Importing Libraries 
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import missingno as msno
# %matplotlib inline
import plotly.express as px
!pip install xgboost

from google.colab import files
uploaded = files.upload()

pd.set_option("display.max_rows", None, "display.max_columns", None)

dataset = pd.read_csv('dataset_small.csv')
df = pd.DataFrame(dataset)

dataset.head(10)

dataset.describe()

dataset.isnull().sum()

from matplotlib import style

phishing = (dataset['phishing'] == 1).sum() 

legit = (dataset['phishing'] == 0).sum()
print(f"Total phishing URL",phishing)
print(f"Total legit URL",legit)
p = [phishing, legit]
plt.pie(p,
       labels = ['Phishing', 'Legit'], 
       colors = ['red', 'green'],  
       explode = (0.15, 0),
       startangle = 0) 
plt.axis('equal') 
plt.show()

from sklearn.feature_selection import VarianceThreshold
var_thres = VarianceThreshold(threshold=0)
var_thres.fit(dataset)
var_thres.get_support()
constant_columns = [column for column in dataset.columns
                    if column not in dataset.columns[var_thres.get_support()]]
print(f"No of columns with 0 variance: {len(constant_columns)}")
constant_columns

dataset = dataset.drop(constant_columns,axis=1)
dataset.shape

#length of dataset before dropping duplicate rows
lengthbeforedropping=len(dataset)
lengthbeforedropping

# plotting count of values per columns ignoring missing values for dataset
msno.bar(dataset,color='dodgerblue', sort='ascending',log=True)

#length of dataset after dropping duplicate rows

dataset.drop_duplicates(keep=False,inplace=True)
lengthafterdropping=len(dataset)
lengthafterdropping

#Duplicate Rows
duplicaterows=lengthbeforedropping-lengthafterdropping
duplicaterows

#Replacing the value -1 with Nan and then deleting those rows

#Finding rows which contain the value -1
dataset.isin(['-1']).count()

#All the rows have the value -1 in atleast one of the columns, so lets remove the rows which have the maximum number of -1

# Data distribution of the features
cols={} 
for i in dataset.columns:
    print("- - - - - New Column Here- - - - - - - ")
    x=dataset[i].value_counts(normalize=True)
    print(x)
    if dataset[i].isin([-1]).any():
        cols[i]=x[-1]

"""Removing the existing -1's and replacing with NAN in order to replace the values using different imputers."""

for i,j in cols.items():
    if j>=0.8:
        dataset.drop(i,inplace=True,axis=1)

df_imp=dataset.replace(to_replace = -1,value =np.nan)

##Visualization of Missing Data using missingno lib.

msno.bar(df_imp,figsize=(20,20))

df_imp

df_imp.isnull().sum()

# visualizing the nullity by column
msno.bar(df_imp, log = True, color = 'g');

msno.heatmap(df_imp,  cmap='GnBu_r');

df_imp.isnull().sum()

"""Having a missing value in a machine learning model is considered very inefficient and hazardous because of the following reasons: Reduces the efficiency of the ML model. Affects the overall distribution of data values. It leads to a biased effect in the estimation of the ML model. Therefore, Now We impute the data with different imputation techiniques which we later might use it for model training. The different imputation techniques which we can use are Mean, Mode, Median, KNN Imputation.

Mean Imputation
"""

from sklearn.impute import SimpleImputer
imp_mean = SimpleImputer( strategy='mean')
imp_mean.fit(df_imp)
mean_imputed_df = imp_mean.transform(df_imp)
mean_imputed_df = pd.DataFrame(mean_imputed_df,columns = df_imp.columns)
mean_imputed_df.shape

mean_imputed_df.describe()

mean_imputed_df.isnull().sum()

msno.bar(mean_imputed_df, log = True, color = 'g');

"""Mode Imputation"""

imp_mode = SimpleImputer( strategy='most_frequent')
imp_mode.fit(df_imp)
mostFreq_imputed_df = imp_mode.transform(df_imp)
mostFreq_imputed_df = pd.DataFrame(mostFreq_imputed_df,columns = df_imp.columns)
mostFreq_imputed_df.shape

mostFreq_imputed_df.describe()

mostFreq_imputed_df.isnull().sum()

msno.bar(mostFreq_imputed_df, log = True, color = 'g');

"""Mode Imputation"""

imp_median = SimpleImputer( strategy='median')
imp_median.fit(df_imp)
median_imputed_df = imp_median.transform(df_imp)
median_imputed_df = pd.DataFrame(median_imputed_df,columns = df_imp.columns)
median_imputed_df.shape

median_imputed_df.describe()

median_imputed_df.isnull().sum()

msno.bar(median_imputed_df, log = True, color = 'g');

